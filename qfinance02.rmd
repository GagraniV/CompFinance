---
title: "The Constant Expected Return Model(CER) in R"
author: "Vijaya Gagrani"
date: "May 2015"
output:
  slidy_presentation: default
  ioslides_presentation:
    keep_md: yes
    transition: faster
    widescreen: yes
  beamer_presentation: default
---


```{r Set_ChunkOptions, echo=FALSE}
#Select Chunk and package options @ http://yihui.name/knitr/options
knitr::opts_chunk$set(comment = NA, echo= FALSE, message = FALSE, fig.align='center', warning = FALSE,cache=FALSE)

```

## Mean, Variance, Standard Deviation, Skewness, and Kurtosis for the four assests
---
Sk:tail thickness
kr:

```{r Initial_Settings_n_Basic_Stat}
#set working directory
#setwd("C:/Users/gagranis/Documents/Finance/Coursera/qfinance")
options(digits=4, width=70)
library("PerformanceAnalytics")
library("xts")
library("zoo")
library("tseries")
library("plyr")
library("mvtnorm")
library("splus2R")
## Monthly Adjusted closing prices are downloaded from yahoo finace for SP500,AAPL,VBLX, and WFC from Febuary 1994 to April 2015 and saved as .csv files.
# read .csv data files
SP500_prices <- read.csv("SP500_prices")
AAPL_prices <- read.csv("AAPL_prices")
VBLTX_prices <- read.csv("VBLTX_prices")
WFC_prices <- read.csv("WFC_prices")

# Rename column name
colnames(SP500_prices)[colnames(SP500_prices)=="AdjClose"] <- "sp500"
colnames(AAPL_prices)[colnames(AAPL_prices)=="AdjClose"] <- "aapl"
colnames(VBLTX_prices)[colnames(VBLTX_prices)=="AdjClose"] <- "vbltx"
colnames(WFC_prices)[colnames(WFC_prices)=="AdjClose"] <- "wfc"
# create merged price data
prices <- join_all(list(VBLTX_prices, SP500_prices, AAPL_prices, WFC_prices), by = 'Index')
#coredata is a generic functions for extracting the core data contained in a (more complex) object and replacing it.
prices_mat = coredata(prices)
## Mean, Variance, Standard Deviation, Skewness, and Kurtosis for the four assests
apply(prices_mat[2:5],2, mean) 
apply(prices_mat[2:5],2, var) 
apply(prices_mat[2:5],2, sd)
apply(prices_mat[2:5],2, skewness)
apply(prices_mat[2:5],2, kurtosis)
```

## Monthly Price, Log monthly Price, and Continous Compounding Return Plots
---
Continous Componding, cc means that the principal is constsantly earning interest, and the interest keeps earning interest.
The cc provides marvelous computaional adavantages(rc=ln(1+r)).Using the natural log and exponstial funcations, it  is easy to scale forward and discounting, scaling over multiple periods, and time consitent. 

```{r Intial_Plots, echo=FALSE}
# create cc returns from montly prices
returns <- log(prices_mat[2:5])
returns <- as.zoo(returns)
returns_lcc <- diff(returns)

par <- par("mar")
#par(mar=c(1,1,1,1))
# plot prices and returns for Microsoft
#
par(mfrow=c(2,1))
plot(prices_mat[,"wfc"], col="blue", lwd=2, ylab="price", main="Monthly Prices on WFC")

plot(log(prices_mat[,"wfc"]),col="blue", lwd=2, ylab="log price")

par(mfrow=c(1,1))
plot(returns_lcc[,"wfc"],ylab="cc return",
     main="Monthly CC Returns on WFC",
     col="green", lwd=2)
abline(h=0)

```

## Simulate Constant Expected Return or CER model for monthly returns on wfc

```{r Simu_Plots, echo=FALSE}
# set model parameters
mu = 0.01097
sd_e = 0.08213
nobs = 100
# generate random numbers for error
set.seed(111)
sim_e= rnorm(nobs, mean=0, sd=sd_e)

#Simulate constant compound (cc) returns
sim_ret = mu + sim_e
# plot simulated returns
par(mfrow=c(1,2))
ts.plot(sim_ret, main="",xlab="months",ylab="return", lwd=2, col="blue")
abline(h=mu)
hist(sim_ret, main="", xlab="returns", col="slateblue1")
par(mfrow=c(1,1))

````

## Graphically summarize empirical distribution of simulated data

``` {r}
par(mfrow=c(2,2))
hist(sim_ret, xlab="return",ylab="frequency", 
     main="Simulated returns from CER model", col="slateblue1")
boxplot(sim_ret, col="slateblue1")
plot(density(sim_ret),type="l",xlab="return",ylab="density",
     col="slateblue1", lwd=2, main="smoothed densiy")
qqnorm(sim_ret, col="slateblue1")
qqline(sim_ret)
par(mfrow=c(1,1))

```

## simulate random walk model (RWM) with initial log price = 1(wfc)
---
The RWM model for log prices, mean: reprsents the expected change in the log prices between months t-1 and t; error term: represents the unexpected change in the log price.
The figure shows the simulated values for log-price based on RW model. The top panel shows the simulated price (sim_pe,p(t), blue color), the expected log price ((1+mu*seq(nobs),E[p(t)], dotted), and the accumulated random news (cumsum(sim_e),p(t)-E[p(t)], orange color). The bottom panel shows the simulated price levels (exp(sim_p),green color).

```{r}
#A Monte Carlo simulation of RW model with p0 = 1
mu = 0.01097
sd_e = 0.08213
nobs = 100
set.seed(111)
sim_e = rnorm(nobs, mean=0, sd=sd_e)
sim_p = 1 + mu*seq(nobs) + cumsum(sim_e)
sim_pe = exp(sim_p)
# to get the orginal prices
par(mfrow=c(2,1))
ts.plot(sim_p, col="blue",lwd=2,
        ylim=c(-2, 4), ylab="log price")
lines( (1+mu*seq(nobs)), lty="dotted", col="black", lwd=2)
lines(cumsum(sim_e), col="orange", lty="dashed", lwd=2)
abline(h=0)
legend(x="topleft",legend=c("p(t)","E[p(t)]","p(t)-E[p(t)]"),
       lty=c("solid","dotted","dashed"), col=c("blue","black","orange"), 
       lwd=2, cex=c(0.75,0.75,0.75))
ts.plot(sim_pe, lwd=2, col="green", ylab="price")
par(mfrow=c(1,1))

```

---

##Plot returns on all assests.

```{r}

# plot returns on wfc, sbux and sp500
#
returns_lcc_m = coredata(returns_lcc)
wfc_ret = returns_lcc[,"wfc"]
aapl_ret = returns_lcc[,"aapl"]
sp500_ret = returns_lcc[,"sp500"]

# panel function to put horizontal lines at zero in each panel
my.panel <- function(...) {
  lines(...)
  abline(h=0)
}
plot(returns_lcc, plot.type="single", lwd=2, col=1:4)
legend(x="bottomleft", legend=colnames(returns_lcc), col=1:4, lwd=2)
abline(h=0)

pairs(returns_lcc_m, col="blue")

```

##estimate parameters (i.e., mean, variance, standard deviation, covariance, correlation) from the CER model
---
Covariance matrix generalizes the notion of variance to multiple dimensions. The variances appear along the diagonal and covariances appear in the off-diagonal elements. The inverse of covariance matrix, is the inverse covariance matrix, also known as the concentration matrix or precision matrix. The elements of the precision matrix have an interpretation in terms of partial correlations and partial variances.

Correlation matrix is the matrix of Pearson product moment correlation coefficients between each of the random variables.

```{r,echo =FALSE}
#mean
(muhat_vals = apply(returns_lcc_m,2,mean))
#var
(sigma2hat_vals = apply(returns_lcc_m,2,var))
#sd
(sigmahat_vals = apply(returns_lcc_m,2,sd))
#covariance matrix:
cov_mat = var(returns_lcc)
#corelation matrix
cor_mat = cor(returns_lcc)

covhat_vals = cov_mat[lower.tri(cov_mat)]
rhohat_vals = cor_mat[lower.tri(cor_mat)]
names(covhat_vals) <- names(rhohat_vals) <- c("sp500,vbltx","aapl,vbltx","wfc,vlltx","aapl,sp500","wfc,sp500","wfc,aapl")
covhat_vals
rhohat_vals
```

##Compute estimated standard deviation (SD) or standard error (SE) and 95% confidence intervals for mean,var, sd, and correlation.
---
Standard Error:
1. Standard error (SE) is the standard deviation of the sampling distribution of a statistic, most commonly of the mean.
SEM: The sample mean is the usual estimator of a population mean,however different samples drawn from that same population would in general have different values of the sample mean, so there is a distribution of sampled means.
2 SEM is the standard deviation of those sample means over all possible samples drawn from the population. 
3. SEM can refer to an estimate of that standard deviation, computed from the sample of data being analyzed at the time.
4. SEM is usually estimated by the sample estimate of the population standard deviation (sample standard deviation) divided by the square root of the sample size.
5. In regression analysis, the term "sd" is used in the phrase standard error of the regression to mean the ordinary least squares estimate of the standard deviation of the underlying errors.

Confidence Interval: It is an interval estimate of mean to put an probability statment about the likelyhood that the interval covers the mean.
(http://spark-public.s3.amazonaws.com/compfinance/Lecture%20Notes/cermodelslides.pdf).

Generally SE of a population is approximated using the Gaussian sample distribution, t-student distribution. T-distribution is highly dependent on sample size, and it use to calculate confidence intervals. Since the data are assumed to be normally distributed, quantiles of the normal distribution and the sample mean and standard error can be used to calculate approximate confidence intervals for the mean. In this example t.975 is the +-95% confidence limit for 1.96 sd of the normal distribution.

---

```{r}

nobs = nrow(returns_lcc_m)
#SE of mean (se_muhat)
(se_muhat = sigmahat_vals/sqrt(nobs))

# compute 95% confidence intervals of mean
t.975 = qt(0.975, df=99)
mu_lower <- muhat_vals - t.975*se_muhat
mu_upper <- muhat_vals + t.975*se_muhat
mu_width <- mu_upper - mu_lower
(cbind(mu_lower,mu_upper,mu_width))

# compute estimated standard errors for variance and sd
se_sigma2hat <- sigma2hat_vals/sqrt(nobs/2)
se_sigmahat <- sigmahat_vals/sqrt(2*nobs)
se_sigma2hat
se_sigmahat

# compute 95% confidence intervals for variance and sd
sigma2_lower <- sigma2hat_vals - 2*se_sigma2hat
sigma2_upper <-sigma2hat_vals + 2*se_sigma2hat
sigma2_width <- sigma2_upper - sigma2_lower
cbind(sigma2_lower,sigma2_upper,sigma2_width)

sigma_lower <- sigmahat_vals - 2*se_sigmahat
sigma_upper <-sigmahat_vals + 2*se_sigmahat
sigma_width <- sigma_upper - sigma_lower
cbind(sigma_lower,sigma_upper,sigma_width)

# compute estimated standard errors for correlation coefficents
se_rhohat <- (1-rhohat_vals^2)/sqrt(nobs)
se_rhohat
# compute approx 95% confidence intervals
rho_lower <- rhohat_vals - 2*se_rhohat
rho_upper <- rhohat_vals + 2*se_rhohat
rho_width <- rho_upper - rho_lower
cbind(rho_lower,rho_upper,rho_width)

```

### Monte Carlo evaluation of unbiasedness
---
In monte Carlo simulation,samples are derived from a random sample of size n for sampling distribution of statistic such as mean, sd, variance, confidence interval. Monte Carlo is also considered as the distribution of the statistic for all possible samples from the same population of a given size.

```{r }

# Generate 1000 samples from CER and evaluate sampling properties of muhat
mu = 0.05
sd = 0.10
n_obs = 100
n_sim = 1000
set.seed(111)
sim_means = rep(0,n_sim)       # initialize vectors
mu_lower = rep(0,n_sim)
mu_upper = rep(0,n_sim)
qt.975 = qt(0.975, n_obs-1)
[] 
for (sim in 1:n_sim) {
        sim_ret = rnorm(n_obs,mean=mu,sd=sd)
        sim_means[sim] = mean(sim_ret)
        se_muhat = sd(sim_ret)/sqrt(n_obs)
        mu_lower[sim] = sim_means[sim]-qt.975*se_muhat
        mu_upper[sim] = sim_means[sim]+qt.975*se_muhat
}
mean(sim_means)
sd(sim_means)
in_interval = mu >= mu_lower & mu <= mu_upper
sum(in_interval)/n_sim
  
hist(sim_means, col="slateblue1", ylim=c(0,40), main="", xlab="muhat", probability=T)
abline(v=mean(sim_means), col="white", lwd=4, lty=2)
# overlay normal curve
x_vals = seq(0.02, 0.08, length=100)
lines(x_vals, dnorm(x_vals, mean=mu, sd=sd/sqrt(100)), col="orange", lwd=2)


```

### Compute expected value of estimates and bias for mean, variance, and SD


```{r}
#generate 1000 samples from CER and compute sample statistics
  
mean(sim_means)
mean(sim_means) - mu
sd(sim_means)
  
se_muhat["aapl"]
mu = 0.05
sd = 0.10
n_obs = 100
n_sim = 1000
set.seed(111)
sim_means = rep(0,n_sim)       # initialize vectors
sim_vars = rep(0,n_sim)
sim_sds = rep(0,n_sim)
for (sim in 1:n_sim) {
        sim_ret = rnorm(n_obs,mean=mu,sd=sd)
        sim_means[sim] = mean(sim_ret)
        sim_vars[sim] = var(sim_ret)
        sim_sds[sim] = sqrt(sim_vars[sim])
}
  
# compute expected value of estimates and bias
mean(sim_means)
mean(sim_means) - mu
mean(sim_vars)
mean(sim_vars) - sd^2
mean(sim_sds)
mean(sim_sds) - sd
  
par(mfrow=c(2,2))
        hist(sim_means, col="slateblue1", xlab="mu hat values", main="mu hat")
        abline(v=mean(sim_means), col="white", lwd=4, lty=2)
        hist(sim_vars, col="slateblue1", xlab="sigma2 hat values", main="sigma2 hat")
        abline(v=mean(sim_vars), col="white", lwd=4, lty=2)
        hist(sim_sds, col="slateblue1", xlab="sigma hat values", main="sigma hat")
        abline(v=mean(sim_sds), col="white", lwd=4, lty=2)
par(mfrow=c(1,1))

```

###Simulate data for three asset returns

```{r}


```

