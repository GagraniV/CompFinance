---
title: "The Constant Expected Return Model(CER) in R"
author: "Vijaya Gagrani"
date: "May 2015"
output:
  slidy_presentation: default
  ioslides_presentation:
    keep_md: yes
    transition: faster
    widescreen: yes
  beamer_presentation: default
---

## Background and Objective

The Constant Expected Return Model or CER is a very simple asset return model can be represented as a simple regression model(CER=Mean+Error). The main assumption is that the asset return is independent and normally distributed over time with a constant mean, variance, covariance, and correlation. Expected return is constant with a normally distributed error term or unexpected news of mean zero and constant variance.
 
For the logarithm of asset prices, the CER model is represented as Random Walk Model or RWM. In addition to the expected return and unexpected news, RWM includes unexpected change in the log-price, which is uncorrelated over time. The change in log-prices is a non-stationary process because variance increases with time t.

http://faculty.washington.edu/ezivot/econ424/constantexpectedreturn.pdf

```{r Set_ChunkOptions, echo=FALSE}
#Select Chunk and package options @ http://yihui.name/knitr/options
knitr::opts_chunk$set(comment = NA, echo= FALSE, message = FALSE, fig.align='center', warning = FALSE,cache=FALSE)

```

## Mean, Variance, Standard Deviation, Skewness, and Kurtosis for the four assests
---
Kurtosis: it measures tail thickness(Leptokurtosis or postive Kr: heavier tails, more concentrted around mean,with thinner flanks, and outlier prone distribution; Platykurtosis or negative Kr:Data points are less heavily concentrated around the mean,tails are also lighter than normal distribution, heavier flanks (looks like a disk)).
Skewness:It measures the asymmetry of the distribution. The direction of skewness is the direction to which the data is trailing off.
Normal probability plot: It is a visual method for determining whether or not data comes from a distribution that is approximately normal. The vertical axis represents the actual data values, and the horizontal axis displays the expected percentiles from a standard normal distribution. The normal reference line along the diagonal is where the data would fall if it were perfectly normal. 


```{r Initial_Settings_n_Basic_Stat}
#set working directory
#setwd("C:/Users/gagranis/Documents/Finance/Coursera/qfinance")
options(digits=4, width=70)
library("PerformanceAnalytics")
library("xts")
library("zoo")
library("tseries")
library("plyr")
library("mvtnorm")
library("splusTimeSeries")
library("boot")


## Monthly Adjusted closing prices are downloaded from yahoo finace for SP500,AAPL,VBLX, and WFC from Febuary 1994 to April 2015 and saved as .csv files.
# read .csv data files
SP500_prices <- read.csv("SP500_prices")
AAPL_prices <- read.csv("AAPL_prices")
VBLTX_prices <- read.csv("VBLTX_prices")
WFC_prices <- read.csv("WFC_prices")

# Rename column name
colnames(SP500_prices)[colnames(SP500_prices)=="AdjClose"] <- "sp500"
colnames(AAPL_prices)[colnames(AAPL_prices)=="AdjClose"] <- "aapl"
colnames(VBLTX_prices)[colnames(VBLTX_prices)=="AdjClose"] <- "vbltx"
colnames(WFC_prices)[colnames(WFC_prices)=="AdjClose"] <- "wfc"
# create merged price data
prices <- join_all(list(VBLTX_prices, SP500_prices, AAPL_prices, WFC_prices), by = 'Index')

#coredata is a generic functions for extracting the core data contained in a (more complex) object and replacing it.
prices_mat = coredata(prices)
## Mean, Variance, Standard Deviation, Skewness, and Kurtosis for the four assests
apply(prices_mat[2:5],2, mean) 
apply(prices_mat[2:5],2, var) 
apply(prices_mat[2:5],2, sd)
apply(prices_mat[2:5],2, skewness)
apply(prices_mat[2:5],2, kurtosis)

```

## Monthly Price, Log monthly Price, and Continous Compounding Return Plots
---
Continous Componding, cc means that the principal is constsantly earning interest, and the interest keeps earning interest.
The cc provides marvelous computaional adavantages(rc=ln(1+r)).Using the natural log and exponstial funcations, it  is easy to scale forward and discounting, scaling over multiple periods, as well as its time consitent. 

```{r Intial_Plots, echo=FALSE}
# create cc returns from montly prices

returns <- log(prices_mat[2:5])
returns <- as.zoo(returns)
returns_lcc <- diff(returns)
returns_lcc<-as.zoo(returns_lcc)

## solution to the error message,"Error in plot.new() : figure margins too large"
par <- par("mar")
#par(mar=c(1,1,1,1))
# plot prices and returns for Microsoft
#
par(mfrow=c(2,1))
plot(prices_mat[,"wfc"], col="blue", lwd=2, ylab="price", main="Monthly Prices on WFC")

plot(log(prices_mat[,"wfc"]),col="blue", lwd=2, ylab="log price")

par(mfrow=c(1,1))
plot(returns_lcc[,"wfc"],ylab="cc return",
     main="Monthly CC Returns on WFC",
     col="green", lwd=2)
abline(h=0)

```

## Simulate Constant Expected Return or CER model for monthly returns on wfc

```{r Simu_Plots, echo=FALSE}
# set model parameters
mu = 0.01097
sd_e = 0.08213
nobs = 100
# generate random numbers for error
set.seed(111)
sim_e= rnorm(nobs, mean=0, sd=sd_e)

#Simulate constant compound (cc) returns
sim_ret = mu + sim_e
# plot simulated returns
par(mfrow=c(1,2))
ts.plot(sim_ret, main="",xlab="months",ylab="return", lwd=2, col="blue")
abline(h=mu)
hist(sim_ret, main="", xlab="returns", col="slateblue1")
par(mfrow=c(1,1))

````

## Graphically summarize empirical distribution of simulated data

``` {r}
par(mfrow=c(2,2))
hist(sim_ret, xlab="return",ylab="frequency", 
     main="Simulated returns from CER model", col="slateblue1")
boxplot(sim_ret, col="slateblue1")
plot(density(sim_ret),type="l",xlab="return",ylab="density",
     col="slateblue1", lwd=2, main="smoothed densiy")
qqnorm(sim_ret, col="slateblue1")
qqline(sim_ret)
par(mfrow=c(1,1))

```

## Simulate random walk model (RWM) with initial log price = 1(wfc)
---
The RWM model for log prices, mean: reprsents the expected change in the log prices between months t-1 and t; 
error term: represents the unexpected change in the log price.
The figure shows the simulated values for log-price based on RW model. The top panel shows the simulated price (sim_pe,p(t), blue color), the expected log price ((1+mu*seq(nobs),E[p(t)], dotted), and the accumulated random news (cumsum(sim_e),p(t)-E[p(t)], orange color). The bottom panel shows the simulated price levels (exp(sim_p),green color).

```{r}
#A Monte Carlo simulation of RW model with p0 = 1
mu = 0.01097
sd_e = 0.08213
nobs = 100
set.seed(111)
sim_e = rnorm(nobs, mean=0, sd=sd_e)
sim_p = 1 + mu*seq(nobs) + cumsum(sim_e)
sim_pe = exp(sim_p)
# to get the orginal prices
par(mfrow=c(2,1))
ts.plot(sim_p, col="blue",lwd=2,
        ylim=c(-2, 4), ylab="log price")
lines( (1+mu*seq(nobs)), lty="dotted", col="black", lwd=2)
lines(cumsum(sim_e), col="orange", lty="dashed", lwd=2)
abline(h=0)
legend(x="topleft",legend=c("p(t)","E[p(t)]","p(t)-E[p(t)]"),
       lty=c("solid","dotted","dashed"), col=c("blue","black","orange"), 
       lwd=2, cex=c(0.75,0.75,0.75))
ts.plot(sim_pe, lwd=2, col="green", ylab="price")
par(mfrow=c(1,1))

```

---

## Plot returns on all assests

```{r}

# plot returns on wfc, sbux and sp500
#
returns_lcc_m = coredata(returns_lcc)
wfc_ret = returns_lcc[,"wfc"]
aapl_ret = returns_lcc[,"aapl"]
sp500_ret = returns_lcc[,"sp500"]

# panel function to put horizontal lines at zero in each panel
my.panel <- function(...) {
  lines(...)
  abline(h=0)
}
plot(returns_lcc, plot.type="single", lwd=2, col=1:4)
legend(x="bottomleft", legend=colnames(returns_lcc), col=1:4, lwd=2)
abline(h=0)

pairs(returns_lcc_m, col="blue")

```

## Estimate parameters (i.e., mean, variance, standard deviation, covariance, correlation) from the CER model
---
Covariance matrix generalizes the notion of variance to multiple dimensions. The variances appear along the diagonal and covariances appear in the off-diagonal elements. The inverse of covariance matrix, is the inverse covariance matrix, also known as the concentration matrix or precision matrix. The elements of the precision matrix have an interpretation in terms of partial correlations and partial variances.

Correlation matrix is the matrix of Pearson product moment correlation coefficients between each of the random variables.

```{r,echo =FALSE}
#mean
(muhat_vals = apply(returns_lcc_m,2,mean))
#var
(sigma2hat_vals = apply(returns_lcc_m,2,var))
#sd
(sigmahat_vals = apply(returns_lcc_m,2,sd))
#covariance matrix:
cov_mat = var(returns_lcc_m)
#corelation matrix
cor_mat = cor(returns_lcc_m)

covhat_vals = cov_mat[lower.tri(cov_mat)]
rhohat_vals = cor_mat[lower.tri(cor_mat)]
names(covhat_vals) <- names(rhohat_vals) <- c("sp500,vbltx","aapl,vbltx","wfc,vlltx","aapl,sp500","wfc,sp500","wfc,aapl")
covhat_vals
rhohat_vals
```

## Compute estimated standard deviation (SD) or standard error (SE) and 95% confidence intervals for mean,var, sd, and correlation
---
Standard Error:
Standard error (SE) is the standard deviation of the sampling distribution of a statistic, most commonly of the mean. The sample mean is the usual estimator of a population mean,however different samples drawn from that same population would in general have different values of the sample mean, so there is a distribution of sampled means (SEM). It is the standard deviation of those sample means over all possible samples drawn from the population. SEM is also refers to an estimate of that standard deviation, computed from the sample of data being analyzed at the time. SEM is usually estimated by the sample estimate of the population standard deviation (sample standard deviation) divided by the square root of the sample size. In regression analysis, the term "sd" is used in the phrase standard error of the regression to mean the ordinary least squares estimate of the standard deviation of the underlying errors.

Confidence Interval: It is an interval estimate of mean to put an probability statment about the likelyhood that the interval covers the mean(http://spark-public.s3.amazonaws.com/compfinance/Lecture%20Notes/cermodelslides.pdf).

Generally SE of a population is approximated using the Gaussian sample distribution, t-student distribution. T-distribution is highly dependent on sample size, and it use to calculate confidence intervals. Since the data are assumed to be normally distributed, quantiles of the normal distribution and the sample mean and standard error can be used to calculate approximate confidence intervals for the mean. In this example t.975 is the +-95% confidence limit for 1.96 sd of the normal distribution.

---

```{r}

nobs = nrow(returns_lcc_m)
#SE of mean (se_muhat)
(se_muhat = sigmahat_vals/sqrt(nobs))

# compute 95% confidence intervals of mean
t.975 = qt(0.975, df=99)
mu_lower <- muhat_vals - t.975*se_muhat
mu_upper <- muhat_vals + t.975*se_muhat
mu_width <- mu_upper - mu_lower
(cbind(mu_lower,mu_upper,mu_width))

# compute estimated standard errors for variance and sd
se_sigma2hat <- sigma2hat_vals/sqrt(nobs/2)
se_sigmahat <- sigmahat_vals/sqrt(2*nobs)
se_sigma2hat
se_sigmahat

# compute 95% confidence intervals for variance and sd
sigma2_lower <- sigma2hat_vals - 2*se_sigma2hat
sigma2_upper <-sigma2hat_vals + 2*se_sigma2hat
sigma2_width <- sigma2_upper - sigma2_lower
cbind(sigma2_lower,sigma2_upper,sigma2_width)

sigma_lower <- sigmahat_vals - 2*se_sigmahat
sigma_upper <-sigmahat_vals + 2*se_sigmahat
sigma_width <- sigma_upper - sigma_lower
cbind(sigma_lower,sigma_upper,sigma_width)

# compute estimated standard errors for correlation coefficents
se_rhohat <- (1-rhohat_vals^2)/sqrt(nobs)
se_rhohat
# compute approx 95% confidence intervals
rho_lower <- rhohat_vals - 2*se_rhohat
rho_upper <- rhohat_vals + 2*se_rhohat
rho_width <- rho_upper - rho_lower
cbind(rho_lower,rho_upper,rho_width)

```

## Monte Carlo evaluation of unbiasedness
---
In monte Carlo simulation,samples are derived from a random sample of size n for sampling distribution of statistic such as mean, sd, variance, confidence interval. Monte Carlo is also considered as the distribution of the statistic for all possible samples from the same population of a given size.

```{r }

# Generate 1000 samples from CER and evaluate sampling properties of muhat
mu = 0.05
sd = 0.10
n_obs = 100
n_sim = 1000
set.seed(111)
sim_means = rep(0,n_sim)       # initialize vectors
mu_lower = rep(0,n_sim)
mu_upper = rep(0,n_sim)
qt.975 = qt(0.975, n_obs-1)
for (sim in 1:n_sim) {
        sim_ret = rnorm(n_obs,mean=mu,sd=sd)
        sim_means[sim] = mean(sim_ret)
        se_muhat = sd(sim_ret)/sqrt(n_obs)
        mu_lower[sim] = sim_means[sim]-qt.975*se_muhat
        mu_upper[sim] = sim_means[sim]+qt.975*se_muhat
}
mean(sim_means)
sd(sim_means)
in_interval = mu >= mu_lower & mu <= mu_upper
sum(in_interval)/n_sim
  
hist(sim_means, col="slateblue1", ylim=c(0,40), main="", xlab="muhat", probability=T)
abline(v=mean(sim_means), col="white", lwd=4, lty=2)
# overlay normal curve
x_vals = seq(0.02, 0.08, length=100)
lines(x_vals, dnorm(x_vals, mean=mu, sd=sd/sqrt(100)), col="orange", lwd=2)


```

## Compute expected value of estimates and bias for mean, variance, and SD


```{r}
#generate 1000 samples from CER and compute sample statistics
  
mean(sim_means)
mean(sim_means) - mu
sd(sim_means)
  
se_muhat["aapl"]
mu = 0.05
sd = 0.10
n_obs = 100
n_sim = 1000
set.seed(111)
sim_means = rep(0,n_sim)       # initialize vectors
sim_vars = rep(0,n_sim)
sim_sds = rep(0,n_sim)
for (sim in 1:n_sim) {
        sim_ret = rnorm(n_obs,mean=mu,sd=sd)
        sim_means[sim] = mean(sim_ret)
        sim_vars[sim] = var(sim_ret)
        sim_sds[sim] = sqrt(sim_vars[sim])
}
  
# compute expected value of estimates and bias
mean(sim_means)
mean(sim_means) - mu
mean(sim_vars)
mean(sim_vars) - sd^2
mean(sim_sds)
mean(sim_sds) - sd
  
par(mfrow=c(2,2))
        hist(sim_means, col="slateblue1", xlab="mu hat values", main="mu hat")
        abline(v=mean(sim_means), col="white", lwd=4, lty=2)
        hist(sim_vars, col="slateblue1", xlab="sigma2 hat values", main="sigma2 hat")
        abline(v=mean(sim_vars), col="white", lwd=4, lty=2)
        hist(sim_sds, col="slateblue1", xlab="sigma hat values", main="sigma hat")
        abline(v=mean(sim_sds), col="white", lwd=4, lty=2)
par(mfrow=c(1,1))

```

## Simulate data for more than one asset returns 

```{r}

nobs = 100
set.seed(123)
sim_e = rmvnorm(nobs,mean=c(0,0,0,0),sigma=cov_mat)
sim_ret = muhat_vals + sim_e
#colIds(sim.ret) <- paste(colIds(returns.mat),".sim",sep="")
colnames(sim_ret) <- paste(colnames(returns_lcc_m),".sim",sep="")



# plot data and scatterplots
ts.plot(sim_ret,main="Simulated return data",
lty=rep(1,3),lwd=rep(2,3),col=c(1,2,5))
legend(0,-0.2,legend=colnames(sim_ret),
lty=rep(1,3),lwd=rep(2,3),col=c(1,2,5))
abline(h=0)
pairs(sim_ret)

# compare with actual data
ts.plot(returns_lcc_m,main="Actual return data",
lty=rep(1,3),lwd=rep(2,3),col=c(1,2,5))
legend(0,-0.2,legend=colnames(sim_ret),
lty=rep(1,3),lwd=rep(2,3),col=c(1,2,5))
abline(h=0)
pairs(returns_lcc_m)

```

## Generate 1000 samples from CER and compute correlations

```{r}

n_obs = 100
n_sim = 1000
set.seed(111)
sim_corrs = matrix(0,n_sim,6)  # initialize vectors
#colIds(sim_corrs) = c("sbux,msft","sbux,sp500","msft,sp500")

colnames(sim_corrs) <-c("sp500,vbltx","aapl,vbltx","wfc,vlltx","aapl,sp500","wfc,sp500","wfc,aapl")

for (sim in 1:n_sim) {
  sim_ret = rmvnorm(n_obs,mean=muhat_vals,sigma=cov_mat)
  cor_mat = cor(sim_ret)
  sim_corrs[sim,] = cor_mat[lower.tri(cor_mat)]
}

par(mfrow=c(3,2))
	hist(sim_corrs[,1], xlab="rhohat(sp500,vbltx)", col="slateblue1", 
       main="sp500,vbltx")
	abline(v=rhohat_vals[1], lwd=4, col="white")
	hist(sim_corrs[,2], xlab="rhohat(aapl,vbltx)", col="slateblue1", 
       main="aapl,vbltx")
	abline(v=rhohat_vals[2], lwd=4, col="white")
	hist(sim_corrs[,3], xlab="rhohat(wfc,vlltx)", col="slateblue1",
       main="wfc,vlltx")
	abline(v=rhohat_vals[3], lwd=4, col="white")

  hist(sim_corrs[,4], xlab="rhohat(aapl,sp500)", col="slateblue1", 
       main="aapl,sp500")
	abline(v=rhohat_vals[4], lwd=4, col="white")
	hist(sim_corrs[,5], xlab="rhohat(wfc,sp500)", col="slateblue1", 
       main="wfc,sp500")
	abline(v=rhohat_vals[5], lwd=4, col="white")
	hist(sim_corrs[,6], xlab="rhohat(wfc,aapl)", col="slateblue1",
       main="wfc,aapl")
	abline(v=rhohat_vals[6], lwd=4, col="white")
par(mfrow=c(1,1))
```

## Compute rolling mean and SD values: using rolling functions from zoo

---

Rolling analysis of mean and SD is very important in finacial time series data to support the key assumption that the model parameters are constant over time. In this technique, model parametrs are computed over a rolling window of a fixed time. It is a tool to backtest statistical models on historical data to evaluate stability and predictive accuracy.

```{r}

#
class(returns_lcc)
colnames(returns_lcc)
start(returns_lcc)
end(returns_lcc)

wfc_lcc= returns_lcc[,"wfc"]
class(wfc_lcc)
plot(wfc_lcc)


# compute rolling means of width 24 months
roll_mean_24 = rollapply(wfc_lcc, width=24,FUN=mean, align="right")
roll_mean_24
plot(roll_mean_24,main="Rolling 24 month mean estimates")
abline(h=muhat_vals["wfc"])

plot(roll_mean_24, main="WFC returns and rolling 24 month means", col="red")
points(wfc_lcc, col="green",ylim=range(wfc_lcc))
abline(h=muhat_vals["wfc"])

# compute rolling sds of width 24 months 
roll_sd_24 = rollapply(wfc_lcc, width=24,FUN=sd, align="right")
roll_sd_24
plot(roll_sd_24, main="Rolling 24 month SD estimates")
abline(h=sigmahat_vals["wfc"])

plot(roll_sd_24,wfc_lcc,main="WFC returns and rolling 24 month sds")
abline(h=muhat_vals["wfc"])

plot(roll_sd_24, main="WFC returns and rolling  24 month sds", col="red")
points(wfc_lcc, col="green",ylim=range(wfc_lcc))
abline(h=muhat_vals["wfc"])
abline(h=sigmahat_vals["wfc"])
```

##Estimate Quantiles and VaR from CER model

---

Value at Risk or VaR comprises three sections : 1) time period,2) confidence level, and 3) loss amount or percentage.
http://www.investopedia.com/articles/04/092904.asp

```{r}

# use R function qnorm to compute quantiles from standard normal distribution
qhat_05 <- muhat_vals + sigmahat_vals*qnorm(0.05)
qhat_05
qnorm(0.05,muhat_vals,sigmahat_vals)

W0 = 100000
VaRhat_05 = (exp(qhat_05)-1)*W0
VaRhat_05

# Other vVaR example 
muhat = 0.02
sigmahat = 0.10
qhat_05 = muhat + sigmahat*qnorm(0.05)
W0 = 10000
VaRhat_05 = (exp(qhat_05)-1)*W0
VaRhat_05

```

## Bootstrapping in CER model
---

Boothstrapping is a resampling tchnique can be used to generate SE and confidence intervals without using the analytical formulas. Resamples are created by sampling with replacement. It accumulate the results and calculate sample distrbution of the statistcs. This method doesn't need normally distrbuted data of a relatively large sample size. Hence it is widely used in CER model or VaR.

```{r}

prices0<-prices
prices0$Index<-NULL
prices0 = zooreg(data=as.matrix(prices0), start=c(1994,02), end=c(2015,04),frequency=12)
index(prices0) = as.yearmon(index(prices0))

prices0 = as.zoo(prices0)
                      
colnames(prices0)
start(prices0)
end(prices0)

returns_lcc0 = diff(log(prices0))
colnames(returns_lcc0)
start(returns_lcc0)
end(returns_lcc0)

returns_lcc0 = coredata(returns_lcc0)
wfc_ret0 = returns_lcc0[,"wfc"]
aapl_ret0 = returns_lcc0[,"aapl"]
sp500_ret0 = returns_lcc0[,"sp500"]

# compute estimates of mu, sigma for MSFT and rho for MSFT and SP500
#
muhat_wfc = mean(wfc_ret0)
sigmahat_wfc = sd(wfc_ret0)
rhohat_wfc_SP500 = cor(returns_lcc0[,c("wfc","sp500")])
muhat_wfc
sigmahat_wfc
rhohat_wfc_SP500

# plot prices and returns for wfc
par(mfrow=c(2,1))

  plot(prices_mat[,"wfc"], col="blue", lwd=2, ylab="price",
	main="Monthly Prices on WFC")
	plot(log(prices_mat[,"wfc"]),col="green", lwd=2, ylab="log price")
par(mfrow=c(1,1))

plot(returns_lcc0[,"wfc"],ylab="cc return",
     main="Monthly cc returns on WFC",
     col="blue", lwd=2)
abline(h=0)

# brute force bootstrap: wfc mean
muhat_wfc <- mean(wfc_ret0)
sigmahat_wfc <- sd(wfc_ret0)
se_muhat_wfc <- sigmahat_wfc/sqrt(length(wfc_ret0))
rbind(muhat_wfc, se_muhat_wfc)

# Boot mean estimates using R boot package function boot().  Boot requires user-supplied functions that take two arguments: data and an index. The index is created by the boot function and represents random resampling with replacement.  The R package boot calls the estimation function repeatedly (equal to bootstrap replication, R=999). Each time, the data will be the same (wfc_ret), but the bootstrap sample supplied using an integer vector of indexes will be different.http://www.mayin.org/ajayshah/KB/R/documents/boot.html

mean_boot = function(x, idx) {
     ans = mean(x[idx])
     ans
}

wfc_mean_boot = boot(wfc_ret0, statistic = mean_boot, R=999)
class(wfc_mean_boot)
names(wfc_mean_boot)
# compare boot SE with analytic SE
se_muhat_wfc = sigmahat_wfc/sqrt(length(wfc_ret0))
se_muhat_wfc
# plot bootstrap distribution and qq-plot against normal
plot(wfc_mean_boot)

# compute bootstrap confidence intervals from normal approximation
# basic bootstrap method and percentile intervals
boot.ci(wfc_mean_boot, conf = 0.95, type = c("norm","perc"))

# compare boot confidence intervals with analytic confidence interval
wfc_lower = muhat_wfc - 2*se_muhat_wfc
wfc_upper = muhat_wfc + 2*se_muhat_wfc
cbind(wfc_lower,wfc_upper)

# boostrap SD estimate
#
# function for bootstrapping sample standard deviation
sd_boot = function(x, idx) {
     ans = sd(x[idx])
     ans
}
wfc_sd_boot = boot(wfc_ret0, statistic = sd_boot, R=999)
wfc_sd_boot

# compare boot SE with analytic SE
se_sigmahat_wfc = sigmahat_wfc /sqrt(2*length(wfc_ret0))
se_sigmahat_wfc 

# plot bootstrap distribution
plot(wfc_sd_boot)

# compute confidence intervals
boot.ci(wfc_sd_boot, conf=0.95, type=c("norm", "basic", "perc"))

# compare boot confidence intervals with analytic confidence interval
wfc_lower = sigmahat_wfc - 2*se_sigmahat_wfc
wfc_upper = sigmahat_wfc + 2*se_sigmahat_wfc
cbind(wfc_lower,wfc_upper)
# bootstrap correlation

# function to compute correlation between 1st 2 variables in matrix
rho_boot = function(x_mat, idx) {


	ans = cor(x_mat[idx,])[1,2]
	ans
}
wfc_SP500_cor_boot = boot(returns_lcc0[,c("wfc","sp500")], statistic=rho_boot, R = 999)
wfc_SP500_cor_boot
# compare boot SE with analytic SE based on CLT
se_rhohat_wfc_SP500 = (1 - rhohat_wfc_SP500^2)/sqrt(length(wfc_ret0))
se_rhohat_wfc_SP500

# plot bootstrap distribution
plot(wfc_SP500_cor_boot)

# bootstrap confidence intervals
boot.ci(wfc_SP500_cor_boot, conf=0.95, type=c("norm", "basic", "perc"))

# boot estimate of normal distribution quantile

norm_quantile_boot = function(x, idx, p=0.05) {
	q = mean(x[idx]) + sd(x[idx])*qnorm(p)
	q
}

wfc_q05_boot = boot(wfc_ret0, statistic=norm_quantile_boot, R=999)
wfc_q05_boot
plot(wfc_q05_boot)
boot.ci(wfc_q05_boot, conf=0.95, type=c("norm", "basic", "perc"))

# 5% Value-at-Risk
ValueAtRisk_boot = function(x, idx, p=0.05, w=100000) {
	q = mean(x[idx]) + sd(x[idx])*qnorm(p)
	VaR = (exp(q) - 1)*w
	VaR
}

wfc_VaR_boot = boot(wfc_ret0, statistic = ValueAtRisk_boot, R=999)
wfc_VaR_boot
boot.ci(wfc_VaR_boot, conf=0.95, type=c("norm", "perc"))

plot(wfc_VaR_boot)

```

###Hypothesis testing in CER model

Hypothesis test is a mathematical model for testing a claim, idea or hypothesis about a parameter of interest in a given population. It starts by stating and assuming the "Null Hypothesis", and then the process determines whether the assumption is likely to be true or false. Alternative hypothesis is a direct contradiction of the null hypothesis. For a normal distribution, 95% of the values lie within 2 standard deviations of the population mean. Hence, this normal distribution and central limit assumption for the sample dataset allows us to establish 5% as a significance level. Depending upon the nature of datasets, other significance levels can be taken at 1%, 5% or 10%. If we find any calculations that go beyond the usual 2 sd, then we have a strong case of outliers to reject the null hypothesis. There can be two type of erros: Type1 or alpha: selecting the correct critical value allows eliminating the type-1 alpha errors or limiting them to an acceptable range. When the probability of a Type I error is less than 5% (p < 0.05), we decide to reject the null hypothesis; otherwise, we retain the null hypothesis. Type2 or beta:The probability of incorrectly retaining the null hypothesis, when in fact it is not applicable to the entire population.P(identify a Type I alpha error) = P(reject H0 given that H0 is true).  http://www.investopedia.com


```{r hypo}
# chi-square distribution
# Basic significance tests for CER Model
# construct test by brute force
prices00<-prices
prices00$Index<-NULL
prices00 <- zooreg(data=as.matrix(prices00), start=c(1994,02), end=c(2015,04),frequency=12)
index(prices00) <- as.yearmon(index(prices00))

prices00 <- as.zoo(prices00)
colnames(prices00)
start(prices00)
end(prices00)


returns_lcc00 = diff(log(prices00))
colnames(returns_lcc00)
start(returns_lcc00)
end(returns_lcc00)
returns_lcc00 = coredata(returns_lcc00)

nobs <- 254
muhat_vals <- apply(returns_lcc00, 2, mean)
sigmahat_vals <- apply(returns_lcc00, 2, sd)
se_muhat <- sigmahat_vals/sqrt(nobs)
t_stats <- muhat_vals/se_muhat
#abs(t_stats)

# compute 2-sided 5% critical values
cv_2sided <- qt(0.975, df=nobs-1)
#abs(t_stats) <- cv_2sided

# compute 2-sided p-values
(2*(1-pt(abs(t_stats),df=nobs-1)))

# Test H0: mu = 0 for appl
t_test_aapl <- t.test(returns_lcc00[,"aapl"], alternative="two.sided",mu=0, conf.level=0.95)

# Test H0: mu = 0 for wfc and sp500
t.test(returns_lcc00[,"wfc"], alternative="two.sided", mu=0, conf.level=0.95)
t.test(returns_lcc00[,"sp500"], alternative="two.sided",mu=0, conf.level=0.95)

# test for specific value
# Test H0: mu = 0.03 for appl
(t.test(returns_lcc00[,"aapl"], alternative="two.sided",mu = 0.03, conf.level=0.95))

#
# test for sign
#

# Test H0: mu > 0 for appl
(t.test(returns_lcc00[,"aapl"],  alternative="greater",mu = 0, conf.level=0.95))

#
# Paired t-test for equality of means
#

# Test H0: mu_appl = mu_wfc vs. H1: mu_appl /= mu_wfc
(t.test(x=returns_lcc00[,"aapl"],y=returns_lcc00[,"wfc"], paired=T))


# test for normality of returns
par <- par("mar")
par(mar=c(1,1,1,1))
par(mfrow=c(2,2))
  hist(returns_lcc00[,"wfc"],main="wfc monthly cc returns",
	probability=T, ylab="cc return", col="slateblue1")

	boxplot(returns_lcc00[,"wfc"],outchar=T, ylab="cc return",
              col="slateblue1")
	plot(density(returns_lcc00[,"wfc"]),type="l",xlab="cc return",
	     ylab="density estimate", main="Smoothed density",
           lwd=2, col="slateblue1")
	qqnorm(returns_lcc00[,"wfc"], col="slateblue1")
	qqline(coredata(returns_lcc00[,"wfc"]))
par(mfrow=c(1,1))

wfc_skew <- skewness(returns_lcc00[,"wfc"])
wfc_ekurt<-kurtosis(returns_lcc00[,"wfc"])			# note: this is excess kurtosis

JB <- nobs*(wfc_skew^2 + 0.25*wfc_ekurt^2)/6


# compute p-value from chi-square 2 distn

p_value <- (1 - pchisq(JB,df=2))

# use jarque.bera.test() function from tseries package
(jarque.bera.test(returns_lcc00[,"wfc"]))

#
# test for no autocorrelation in returns
#

(wfc_acf <- acf(returns_lcc00[,"wfc"]))

#
# diagnostics for constant parameters
#

#
# compute two sample t-test for equality of means
#

# Split sample into two equally sized pieces
# Test H0: E[r_sample1] = E[r_sample2]

(t.test(x=returns_lcc00[1:50,"aapl"],y=returns_lcc00[51:100,"aapl"],paired=F))


# compute rolling means for wfc
# using function rollapply
# note: must use zoo objects
#

# compute rolling means over 24 month windows
roll_muhat <- rollapply(returns_lcc00[,"wfc"], width=24,FUN=mean, align="right")
roll_muhat[1:5]

# plot rolling estimates with data
plot(merge(roll_muhat,returns_lcc00[,"wfc"]), plot.type="single",
     main="24 month rolling means for wfc",ylab="returns",
     lwd=c(2,2), col=c("blue","orange"))
abline(h=0)
legend(x="bottomleft",legend=c("Rolling mean","Monthly returns"),
       lwd=c(2,2), col=c("blue","orange"))

# compute rolling standard deviations over 24 month windows
roll_sigmahat <- rollapply(returns_lcc00[,"wfc"],width=24,
                          FUN=sd, align="right")
roll_sigmahat[1:5]

# plot rolling estimates with data
plot(merge(roll_sigmahat,returns_lcc00[,"wfc"]), plot.type="single",
     main="24 month rolling SDs for wfc", ylab="Returns",
     lwd=c(2,2), col=c("blue","orange"))
abline(h=0)
legend(x="bottomleft",legend=c("Rolling SD","Monthly returns"),
       lwd=c(2,2), col=c("blue","orange"))

# repeat analysis for sp500
# compute rolling means over 24 month windows
roll_muhat <- rollapply(returns_lcc00[,"sp500"], width=24,
                       FUN=mean, align="right")
roll_muhat[1:5]

# plot rolling estimates with data
plot(merge(roll_muhat,returns_lcc00[,"sp500"]), plot.type="single",
     main="24 month rolling means for SP500",ylab="returns",
     lwd=c(2,2), col=c("blue","orange"))
abline(h=0)
legend(x="bottomleft",legend=c("Rolling mean","Monthly returns"),
       lwd=c(2,2), col=c("blue","orange"))

# compute rolling standard deviations over 24 month windows
roll_sigmahat <- rollapply(returns_lcc00[,"sp500"],width=24,
                          FUN=sd, align="right")
roll_sigmahat[1:5]

# plot rolling estimates with data
plot(merge(roll_sigmahat,returns_lcc00[,"sp500"]), plot.type="single",
     main="24 month rolling SDs for SP500", ylab="Returns",
     lwd=c(2,2), col=c("blue","orange"))
abline(h=0)
legend(x="bottomleft",legend=c("Rolling SD","Monthly returns"),
       lwd=c(2,2), col=c("blue","orange"))

#
# compute rolling correlations
#
rhohat <- function(x) {
	cor(x)[1,2]
}

# compute rolling estimates b/w sp500 and wfc
roll_rhohat <- rollapply(returns_lcc00[,c("sp500","wfc")],
                       width=24,FUN=rhohat, by.column=FALSE,
                       align="right")
roll_rhohat[1:5]

# plot rolling estimates 

plot(roll_rhohat, main="Rolling Correlation b/w SP500 and wfc",
     lwd=2, col="blue", ylab="rho.hat")
abline(h=0)   

# compute rolling correlations b/w wfc and appl
roll_rhohat <- rollapply(returns_lcc00[,c("wfc","aapl")],
                       width=24,FUN=rhohat, by.column=FALSE,
                       align="right")
roll_rhohat[1:5]

plot(roll_rhohat, main="Rolling Correlation b/w wfc and aapl",
     lwd=2, col="blue", ylab="rho_hat")
abline(h=0) 

```

